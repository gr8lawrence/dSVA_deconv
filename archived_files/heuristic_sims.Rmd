---
title: "Initial Simulations"
author: "Tianyi Liu"
date: "2023-06-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.align = 'center', eval = TRUE)
options(scipen = 1, digits = 3)
```


```{r packages, echo=TRUE}
# source("a01_extended_dSVA.R")
source("s02_simulation_functions.R")
library(extraDistr)
library(statmod)
library(MASS)
library(dplyr)
library(tidyr)
library(statmod)
set.seed(100)
```

The original dSVA model assumes
$$
\mathbf{Y} = \mathbf{X}\mathbf{B} + \mathbf{Z}\mathbf{D} + \mathbf{E},
$$
which can have two interpretations in the deconvolution context, depending on the orientation of $\mathbf{Y}$.

## Interpretation 1: Use Signature Matrix to Infer Proportions - Adjusting for Gene-wise Confounders

If we let $\mathbf{Y}$ be the $m \times n$ matrix as in the canonical deconvolution models, where $m$ is the number of genes and $n$ the number of samples, $\mathbf{X}$ is the signature matrix, and $\mathbf{B}$ the proportion matrix. Respectively, $\mathbf{Z}$ will be the matrix of gene-wise unadjusted confounders (since it has $m$ rows) and $\mathbf{D}$ the coefficients. Hence, using $\mathbf{\Theta}$ and $\mathbf{P}$ to denote the signature and proportion matrices, the dSVA model is rewritten as 

$$
\mathbf{Y} = \mathbf{\Theta}\mathbf{P} + \mathbf{Z}\mathbf{D} + \mathbf{E}, \\
\mathbf{\Theta} \geq 0, \mathbf{P} \geq 0, \\
\mathbf{P}^T \mathbf{1} = \mathbf{1}.
$$
This calls for using a constrained non-negative least square to solve for $\mathbf{P}$ in every regression step, which is available through the `pnnls()` function from the `lsei` package.

#### dSVA function: interpretation 1

```{r prop_dsva, echo=TRUE}
## Y: gene-by-sample bulk expression matrix (m x n)
## X: gene-by-cell type signature matrix (m x K), fully known
## q: the presumed number of confounders
dsva_ext <- function(Y, X, q) {
  n <- ncol(Y)
  m <- nrow(Y)
  
  ## step 1: obtain the residual when confounders Z are ignored
  B_star_hat <- apply(Y, 2, function(y) {lsei::pnnls(a = X, b = y, sum = 1)$x}) # solve a constrained NNLS  
  U_x <- svd(X)$u
  M_x <- U_x %*% t(U_x) # projection matrix onto the column space of X
  R <- (diag(1, m) - M_x) %*% (Y - X %*% B_star_hat) # we project the residual to be orthogonal to X
  # norm(R, "F")/norm(Y - X %*% B_star_hat, "F")
  
  ## step 2: svd on the residual space; choose the first q left singular vectors
  U_q <- svd(R)$u[, seq(q)] 
  
  ## step 3: estimate the surrogate variable
  Psi_hat <- apply(R, 2, function(r) {lsei::lsei(a = U_q, b = r)})
  M_jn <- matrix(1/n, n, n)
  D_jn <- diag(1, n) - M_jn
  Gamma_hat <- U_q + X %*% B_star_hat %*% D_jn %*% t(Psi_hat) %*% solve(Psi_hat %*% D_jn %*% t(Psi_hat))  
  
  ## step 4: fitting the model again with the surrogate variable
  X_new <- cbind(Gamma_hat, X)
  B_hat <- apply(Y, 2, function(y) {lsei::pnnls(a = X_new, b = y, k = q, sum = 1)$x})
  P_hat <- B_hat[-seq(q), ]
  P_hat
}
```


#### Simulations

We use $m$ to denote the number of genes, $n$ the number of samples, $K$ the number of cell types, and $p$ the number of hidden confounders. $p_{\rm sig}$ is the average proportion of genes being markers, and expression of markers in the corresponding cell type is $\lambda$ times as high as in the rest of cell types. $\mu$ is the absolute mean of continuous confounding variables. Other parameters were assumed useful initially but haven't been used in the final simulation. We perform $B$ different simulations.

```{r prop_params, echo=TRUE}
m <- 20
n <- 200
K <- 5
p <- 2
p_sig <- 0.5
p_class <- 0.5
rho <- 0.5
lambda <- 5
mu <- 1 # confounding variable means
# S <- 5
B <- 100
```

The simulations require generating a truth for each matrix component. They are generated via:

  * $\mathbf{x}_j^T \sim \Xi \cdot \pi(W \cdot {\rm dir}(1, 1, \dots, 1) + (1 - W) \cdot {\rm dir}(\lambda, 1, \dots, 1) )$, where $\Xi \sim \chi^2_{200}$, $W \sim Ber(p_{\rm sig})$, and $\pi$ is a permutation operator;
  * $\mathbf{z}_1 = (0, \dots. 0, 1, \dots, 1)^T$ ($m/2$ $0$s and $1$s each).
  * $z_{j2} \sim I(j < m/2) \cdot N(-\mu, 100) + I(j \geq m/2) N(\mu, 100)$.
  * $\mathbf{\alpha}_i \sim  {\rm dir}(1, \dots, 1, \rho, \dots, \rho)$ ($K$ $1$s and $p$ $\rho$s);
  * $\mathbf{p}_i \sim {\rm dir}(\alpha_{i1}, \dots, \alpha_{iK})$;
  * $\mathbf{d}_i \sim N(\mathbf{0}, {\rm diag}(\alpha_{i(K + 1)}, \dots, \alpha_{i(K + p)}))$
  * $\mathbf{e}_j^T \sim N(\mathbf{0}, S\mathbf{I}_n)$, wherw $S \sim IG(10, 9)$.

```{r prop_sims, echo=TRUE}
result_matrix <- matrix(nrow = B, ncol = 8)

## run the simulation B times
for (b in 1:B) {
  true_data <- dSVA_deconv_gene(m, n, K, p, p_sig, p_class, rho, lambda, mu)
  P_dSVA <- dsva_ext(Y = true_data$Y, X = true_data$X, q = p)
  P_nnls <- apply(true_data$Y, 2, function(y) {lsei::nnls(a = true_data$X, b = y)$x})
  P_nnls <- apply(P_nnls, 2, function(x) x/sum(x))
  P_pnnls <- apply(true_data$Y, 2, function(y) {lsei::pnnls(a = true_data$X, b = y, sum = 1)$x})
  
  ## if we know both X and Z
  X_new = cbind(true_data$Z, true_data$X)
  P_hat <- apply(true_data$Y, 2, function(y) {lsei::pnnls(a = X_new, b = y, k = p, sum = 1)$x})[-seq(p), ]
  
  cor_dSVA <- mean(my_cor(true_data$P_star, P_dSVA))
  cor_nnls <- mean(my_cor(true_data$P_star, P_nnls))
  cor_pnnls <- mean(my_cor(true_data$P_star, P_pnnls))
  cor_best <- mean(my_cor(true_data$P_star, P_hat))
  
  mse_dSVA <- my_mse(true_data$P_star, P_dSVA)
  mse_nnls <- my_mse(true_data$P_star, P_nnls)
  mse_pnnls <- my_mse(true_data$P_star, P_pnnls)
  mse_best <- my_mse(true_data$P_star, P_hat)
  
  result_matrix[b, ] <- c(cor_dSVA, cor_nnls, cor_pnnls, cor_best, mse_dSVA, mse_nnls, mse_pnnls, mse_best)
}

result_df <- as_tibble(cbind(1:B, result_matrix))
colnames(result_df) <- c("b", "cor_dSVA", "cor_nnls", "cor_pnnls", "cor_known", "mse_dSVA", "mse_nnls", "mse_pnnls", "mse_known")

par(mfrow = c(1, 2))
boxplot(x = result_df$cor_dSVA, result_df$cor_nnls, result_df$cor_pnnls, result_df$cor_known, xlab = "Method", ylab = "Cor")
axis(1, at = c("1", "2", "3", "4"), labels = c("dSVA", "NNLS", "PNNLS", "Known"))
boxplot(result_df$mse_dSVA, result_df$mse_nnls, result_df$mse_pnnls, result_df$mse_known, xlab = "Method", ylab = "MSE")
axis(1, at = c("1", "2", "3", "4"), labels = c("dSVA", "NNLS", "PNNLS", "Known"))
```

#### Issues
Unlike when the coefficients are unconstrained, when sum-to-one on top of the non-negative constraints were imposed to estimating $\mathbf{P}$, the column space of the residual

$$
 \mathbf{R} = \mathbf{Y} - \mathbf{\Theta}\hat{\mathbf{P}}
$$

isn't usually orthogonal to $C(\mathbf{\Theta})$, as dSVA naturally assumes. In the algorithm, we only use the space in $C(\mathbf{R})$ that is orthogonal to $C(\mathbf{\Theta})$. It can probably invalidate further analysis in dSVA. Noticeably, the original model assume
$$
  \mathbf{Y} = \mathbf{\Theta}\mathbf{P} + \mathbf{Z}\mathbf{D} + \mathbf{E},
$$
where $\mathbb{E}[\mathbf{E}] = \mathbf{0}$. Whereas if we project the residual via

$$
  \mathbf{Y} = \mathbf{\Theta}\hat{\mathbf{P^*}} + \mathbf{R} \\
  = \mathbf{\Theta}\hat{\mathbf{P^*}} + (\mathbf{I_m} - \mathbf{M}_{\mathbf{\Theta}})\mathbf{R} + \mathbf{M}_{\mathbf{\Theta}}\mathbf{R},
$$

the "leftover" residual has $\mathbb{E}[\mathbf{M}_{\mathbf{\Theta}}\mathbf{R}] \neq \mathbf{0}$. This could largely be due to that $C(\mathbf{\Theta})$ isn't orthogonal to $C(\mathbf{R})$.

``` {r orthogonal.residuals.constrained.case, echo=TRUE}
ls <- dSVA_deconv_gene(m, n, K, p, p_sig, p_class, rho, lambda, mu)
Y <- ls$Y
X <- ls$X
m <- nrow(Y)
n <- ncol(Y)
  
## step 1: obtain the canonical model residual 
B_star_hat <- apply(Y, 2, function(y) {lsei::pnnls(a = X, b = y, sum = 1)$x})
R_star <- Y - X %*% B_star_hat
(t(X) %*% R_star)[, 1:10]
U_x <- svd(X)$u
mean(U_x %*% t(U_x) %*%  R_star) # mean left-over residual value
```

Otherwise, we need the assumption that $C(\mathbf{\Theta}) \perp C(\mathbf{R})$, either non-asymptotically or asymptotically as $m \rightarrow \infty$.

![Why orthogonality of residuals fails in constrained NNLS case](images/sketch1.jpeg)

However, barring some boundary conditions, this issue is half solved if we remove the sum-to-one constraint in this model, i.e. we use relative proportions, or we use the other interpretation of the dSVA model, to obtain the cell-type specific expression (see next).

![Why orthogonality of residuals may be half solved in NNLS case](images/sketch2.jpeg)

This is only the case when $m = K = 2$, If we generalize the issue to higher dimensions, say we have $K = 2$ and $m = 200$, still only one of the orthogonality can be broken when $\mathbf{y}$ falls outside the cone once projected to the two dimensional plane spanned by $(\mathbf{\theta}_1, \mathbf{\theta}_2)$. There are conditions though, that this could be solved. For example, $m = K = 2$, $\mathbf{\theta}_1 = (c_1, 0)^T$,  $\mathbf{\theta}_2 = (0, c_2)^T$, this issue is solved. In higher dimension cases, this means that a $\mathbf{\theta}_1$ and $\mathbf{\theta}_2$ span a 2-dimensional positive orthant of $\mathbb{R}^m$.

### Reomve the sum-to-one constraint when solving for $\mathbf{P}$.

```{r prop_nn_only, echo=TRUE}
dsva_ext_nn <- function(Y, X, q) {
  n <- ncol(Y)
  m <- nrow(Y)
  
  ## step 1: obtain the residual when confounders Z are ignored
  B_star_hat <- apply(Y, 2, function(y) {lsei::nnls(a = X, b = y)$x}) # solve an NNLS
  U_x <- svd(X)$u
  M_x <- U_x %*% t(U_x) # projection matrix onto the column space of X
  R <- (diag(1, m) - M_x) %*% (Y - X %*% B_star_hat) # we project the residual to be orthogonal to X
  # norm(R, "F")/norm(Y - X %*% B_star_hat, "F")
  
  ## step 2: svd on the residual space; choose the first q left singular vectors
  U_q <- svd(R)$u[, seq(q)] 
  
  ## step 3: estimate the surrogate variable
  Psi_hat <- apply(R, 2, function(r) {lsei::lsei(a = U_q, b = r)})
  M_jn <- matrix(1/n, n, n)
  D_jn <- diag(1, n) - M_jn
  Gamma_hat <- U_q + X %*% B_star_hat %*% D_jn %*% t(Psi_hat) %*% solve(Psi_hat %*% D_jn %*% t(Psi_hat))  
  
  ## step 4: fitting the model again with the surrogate variable
  X_new <- cbind(Gamma_hat, X)
  B_hat <- apply(Y, 2, function(y) {lsei::pnnls(a = X_new, b = y, k = q, sum = NULL)$x})
  P_tilde <- B_hat[-seq(q), ]
  P_hat <- apply(P_tilde, 2, function(x) x/sum(x))
  P_hat
}
```

Let us see whether the orthogonal residual has been improved

``` {r orthogonal.residuals.unconstrained.case, echo=TRUE}
ls <- dSVA_deconv_gene(m, n, K, p, p_sig, p_class, rho, lambda, mu)
Y <- ls$Y
X <- ls$X
m <- nrow(Y)
n <- ncol(Y)
  
## step 1: obtain the canonical model residual 
B_star_hat <- apply(Y, 2, function(y) {lsei::nnls(a = X, b = y)$x})
# B_star_hat <- apply(B_star_hat, 2, function(b) b/sum(b))
R_star <- Y - X %*% B_star_hat
(t(X) %*% R_star)[, 1:10]
U_x <- svd(X)$u
mean(U_x %*% t(U_x) %*%  R_star) # mean left-over residual value
```
The mean residual has not been reduced closer to 0 (because we did not fit these parameters according to the new model), although the orthogonality has improved in some directions.

```{r prop_sims_nn_only, echo=TRUE}
result_matrix <- matrix(nrow = B, ncol = 8)

## run the simulation B times
for (b in 1:B) {
  true_data <- dSVA_deconv_gene(m, n, K, p, p_sig, p_class, rho, lambda, mu)
  P_dSVA <- dsva_ext_nn(Y = true_data$Y, X = true_data$X, q = p)
  P_nnls <- apply(true_data$Y, 2, function(y) {lsei::nnls(a = true_data$X, b = y)$x})
  P_nnls <- apply(P_nnls, 2, function(x) x/sum(x))
  P_pnnls <- apply(true_data$Y, 2, function(y) {lsei::pnnls(a = true_data$X, b = y, sum = 1)$x})
  
  ## if we know both X and Z
  X_new = cbind(true_data$Z, true_data$X)
  P_hat <- apply(true_data$Y, 2, function(y) {lsei::pnnls(a = X_new, b = y, k = p, sum = 1)$x})[-seq(p), ]
  
  cor_dSVA <- mean(my_cor(true_data$P_star, P_dSVA))
  cor_nnls <- mean(my_cor(true_data$P_star, P_nnls))
  cor_pnnls <- mean(my_cor(true_data$P_star, P_pnnls))
  cor_best <- mean(my_cor(true_data$P_star, P_hat))
  
  mse_dSVA <- my_mse(true_data$P_star, P_dSVA)
  mse_nnls <- my_mse(true_data$P_star, P_nnls)
  mse_pnnls <- my_mse(true_data$P_star, P_pnnls)
  mse_best <- my_mse(true_data$P_star, P_hat)
  
  result_matrix[b, ] <- c(cor_dSVA, cor_nnls, cor_pnnls, cor_best, mse_dSVA, mse_nnls, mse_pnnls, mse_best)
}

result_df <- as_tibble(cbind(1:B, result_matrix))
colnames(result_df) <- c("b", "cor_dSVA", "cor_nnls", "cor_pnnls", "cor_known", "mse_dSVA", "mse_nnls", "mse_pnnls", "mse_known")

par(mfrow = c(1, 2))
boxplot(x = result_df$cor_dSVA, result_df$cor_nnls, result_df$cor_pnnls, result_df$cor_known, xlab = "Method", ylab = "Cor")
axis(1, at = c("1", "2", "3", "4"), labels = c("dSVA", "NNLS", "PNNLS", "Known"))
boxplot(result_df$mse_dSVA, result_df$mse_nnls, result_df$mse_pnnls, result_df$mse_known, xlab = "Method", ylab = "MSE")
axis(1, at = c("1", "2", "3", "4"), labels = c("dSVA", "NNLS", "PNNLS", "Known"))
```

#### Issue 2

Another issue is that, while there might be variation in the residuals orthogonal to $C(\mathbf{\Theta})$ that can be explained by the hidden confounders, the large scale difference between $\mathbf{\Theta}$ and $\mathbf{P}$ will absorb the need for this correction from plainly using the NNLS or constrained NNLS algorithms. What if, as in some cases, each gene is normalized to have a total expression 1 in all $K$ cell types in the given signature matrix?

#### Rescale each row of $\mathbf{Theta}$ to have a sum of one:

First we row-scale the signature matrix. I think this almost gives way to the CAM method.

```{r prop_sims_nn_only_row_normalize, echo=TRUE}
result_matrix <- matrix(nrow = B, ncol = 8)

## run the simulation B times
for (b in 1:B) {
  true_data <- dSVA_deconv_gene(m, n, K, p, p_sig, p_class, rho, lambda, mu)
  X_rs <- t(apply(true_data$X, 1, function(x) x/sum(x)))
  P_dSVA <- dsva_ext_nn(Y = true_data$Y, X = X_rs, q = p)
  P_nnls <- apply(true_data$Y, 2, function(y) {lsei::nnls(a = X_rs, b = y)$x})
  P_nnls <- apply(P_nnls, 2, function(x) x/sum(x))
  P_pnnls <- apply(true_data$Y, 2, function(y) {lsei::pnnls(a = X_rs, b = y, sum = 1)$x})
  
  ## if we know both X and Z
  X_new = cbind(true_data$Z, X_rs)
  P_hat <- apply(true_data$Y, 2, function(y) {lsei::pnnls(a = X_new, b = y, k = p, sum = 1)$x})[-seq(p), ]
  
  cor_dSVA <- mean(my_cor(true_data$P_star, P_dSVA))
  cor_nnls <- mean(my_cor(true_data$P_star, P_nnls))
  cor_pnnls <- mean(my_cor(true_data$P_star, P_pnnls))
  cor_best <- mean(my_cor(true_data$P_star, P_hat))
  
  mse_dSVA <- my_mse(true_data$P_star, P_dSVA)
  mse_nnls <- my_mse(true_data$P_star, P_nnls)
  mse_pnnls <- my_mse(true_data$P_star, P_pnnls)
  mse_best <- my_mse(true_data$P_star, P_hat)
  
  result_matrix[b, ] <- c(cor_dSVA, cor_nnls, cor_pnnls, cor_best, mse_dSVA, mse_nnls, mse_pnnls, mse_best)
}

result_df <- as_tibble(cbind(1:B, result_matrix))
colnames(result_df) <- c("b", "cor_dSVA", "cor_nnls", "cor_pnnls", "cor_known", "mse_dSVA", "mse_nnls", "mse_pnnls", "mse_known")

par(mfrow = c(1, 2))
boxplot(x = result_df$cor_dSVA, result_df$cor_nnls, result_df$cor_pnnls, result_df$cor_known, xlab = "Method", ylab = "Cor")
axis(1, at = c("1", "2", "3", "4"), labels = c("dSVA", "NNLS", "PNNLS", "Known"))
boxplot(result_df$mse_dSVA, result_df$mse_nnls, result_df$mse_pnnls, result_df$mse_known, xlab = "Method", ylab = "MSE")
axis(1, at = c("1", "2", "3", "4"), labels = c("dSVA", "NNLS", "PNNLS", "Known"))
```

#### Rescale each column of $\mathbf{Theta}$ to have a sum of one:

Then, we column-scale the signature matrix.

```{r prop_sims_nn_only_col_normalize, echo=TRUE}
result_matrix <- matrix(nrow = B, ncol = 8)

## run the simulation B times
for (b in 1:B) {
  true_data <- dSVA_deconv_gene(m, n, K, p, p_sig, p_class, rho, lambda, mu)
  X_cs <- apply(true_data$X, 2, function(x) x/sum(x))
  P_dSVA <- dsva_ext_nn(Y = true_data$Y, X = X_cs, q = p)
  P_nnls <- apply(true_data$Y, 2, function(y) {lsei::nnls(a = X_cs, b = y)$x})
  P_nnls <- apply(P_nnls, 2, function(x) x/sum(x))
  P_pnnls <- apply(true_data$Y, 2, function(y) {lsei::pnnls(a = X_cs, b = y, sum = 1)$x})
  
  ## if we know both X and Z
  X_new = cbind(true_data$Z, X_cs)
  P_hat <- apply(true_data$Y, 2, function(y) {lsei::pnnls(a = X_new, b = y, k = p, sum = 1)$x})[-seq(p), ]
  
  cor_dSVA <- mean(my_cor(true_data$P_star, P_dSVA))
  cor_nnls <- mean(my_cor(true_data$P_star, P_nnls))
  cor_pnnls <- mean(my_cor(true_data$P_star, P_pnnls))
  cor_best <- mean(my_cor(true_data$P_star, P_hat))
  
  mse_dSVA <- my_mse(true_data$P_star, P_dSVA)
  mse_nnls <- my_mse(true_data$P_star, P_nnls)
  mse_pnnls <- my_mse(true_data$P_star, P_pnnls)
  mse_best <- my_mse(true_data$P_star, P_hat)
  
  result_matrix[b, ] <- c(cor_dSVA, cor_nnls, cor_pnnls, cor_best, mse_dSVA, mse_nnls, mse_pnnls, mse_best)
}

result_df <- as_tibble(cbind(1:B, result_matrix))
colnames(result_df) <- c("b", "cor_dSVA", "cor_nnls", "cor_pnnls", "cor_known", "mse_dSVA", "mse_nnls", "mse_pnnls", "mse_known")

par(mfrow = c(1, 2))
boxplot(x = result_df$cor_dSVA, result_df$cor_nnls, result_df$cor_pnnls, result_df$cor_known, xlab = "Method", ylab = "Cor")
axis(1, at = c("1", "2", "3", "4"), labels = c("dSVA", "NNLS", "PNNLS", "Known"))
boxplot(result_df$mse_dSVA, result_df$mse_nnls, result_df$mse_pnnls, result_df$mse_known, xlab = "Method", ylab = "MSE")
axis(1, at = c("1", "2", "3", "4"), labels = c("dSVA", "NNLS", "PNNLS", "Known"))
```

## Interpretation 2: Use Proportions to Infer Cell-type Specific Expression - Adjusting for Sample-wise Confounders

This interpretation flips the bulk matrix into an $n \times m$ sample-by-gene matrix $\mathbf{Y}^T$. Correspondingly, the dSVA equation becomes one for estimating the cell-type specific expression for each gene, while the confounders $\mathbf{Z}$ become those regarding samples. This is the *original* configuration of dSVA.

$$
\mathbf{Y}^T = \mathbf{P}^T\mathbf{\Theta}^T + \mathbf{Z}\mathbf{D} + \mathbf{E}, \\
\mathbf{\Theta} \geq 0, \mathbf{P} \geq 0, \\
\mathbf{P}^T \mathbf{1} = \mathbf{1}.
$$
The algorithm for estimating $\mathbf{\Theta}^T$ given $\mathbf{P}$ is given below. 

```{r gene_dSVA, echo=TRUE}
## Use dsva to impute cell-type specific expression adjusting for the hidden covariates
## X is the proportion matrix: each row represents a samples
dsva_ext2 <- function(Y, X, q) {
  n <- nrow(Y)
  m <- ncol(Y)
  
  ## step 1: obtain the canonical model residual 
  B_star_hat <- apply(Y, 2, function(y) {lsei::nnls(a = X, b = y)$x})
  U_x <- svd(X)$u
  M_x <- U_x %*% t(U_x) 
  # R <- Y - X %*% B_star_hat # challenge No.1: the residual space isn't orthogonal to columns of X
  R <- (diag(1, n) - M_x) %*% (Y - X %*% B_star_hat) # we project the residual to be orthogonal to X
  
  ## step 2: svd on the residual space
  U_q <- svd(R)$u[, seq(q)] 
  
  ## step 3: estimate the surrogate variable
  Psi_hat <- apply(R, 2, function(r) {lsei::lsei(a = U_q, b = r)})
  M_jn <- matrix(1/m, m, m)
  D_jn <- diag(1, m) - M_jn
  Gamma_hat <- U_q + X %*% B_star_hat %*% D_jn %*% t(Psi_hat) %*% solve(Psi_hat %*% D_jn %*% t(Psi_hat))  
  
  ## step 4: fitting the model again with the surrogate variable
  X_new <- cbind(Gamma_hat, X)
  B_hat <- apply(Y, 2, function(y) {lsei::pnnls(a = X_new, b = y, k = q, sum = NULL)$x})
  Theta_hat <- B_hat[-seq(q), ]
  Theta_hat
}
```

The simulation for truths is going to be a little different, The matrices are simulated mostly in the same way as above, except for

  * $\mathbf{p}_i \sim {\rm dir}(\alpha_{i1}, \dots, \alpha_{iK})$;
  * $\mathbf{d}_i \sim N(\mathbf{0}, {\rm diag}(\beta_{i1}, \dots, \beta_{ip}))$,
  
and that those vectors of length $n$ before are now of length $m$ and vice versa.

```{r gene_params, echo=TRUE}
n <- 20
m <- 200
K <- 5
p <- 2
p_sig <- 0.5
p_class <- 0.5
rho <- 0.5
lambda <- 5
mu <- 1 # confounding variable means
# S <- 5
B <- 100
```

First, let us see that the residual from Step 1 is indeed mostly orthogonal to $C(\mathbf{P}^T)$, and the residuals have a mean close to 0:
``` {r orthogonal.residuals, echo=TRUE}
ls <- dSVA_deconv_sim_people(m, n, K, p, p_sig, p_class, rho, lambda, mu)
Y <- ls$Y
X <- ls$P
n <- nrow(Y)
m <- ncol(Y)
  
## step 1: obtain the canonical model residual 
B_star_hat <- apply(Y, 2, function(y) {lsei::nnls(a = X, b = y)$x})
# B_star_hat2 <- apply(Y, 2, function(y) {lsei::lsei(a = X, b = y)})
# B_star_hat <- apply(Y, 2, function(y) {lsei::nnls(a = X, b = y)$x})
R_star <- Y - X %*% B_star_hat
(t(X) %*% R_star)[, 1:10]
U_x <- svd(X)$u
mean(U_x %*% t(U_x) %*%  R_star) # mean left-over residual value
# norm(t(X) %*% R_star, "F")^2/(ncol(X) * nrow(R_star)) # the frobenius norm of R
```

This situation is clearly more ideal.

```{r gene_sims, echo=TRUE}
result_matrix <- matrix(nrow = B, ncol = 6)
ab_list <- list()
bb <- 1 # counting for abnormal outputs
for (b in 1:B) {
  true_data <- dSVA_deconv_sim_people(m, n, K, p, p_sig, p_class, rho, lambda, mu)
  X_dSVA <- dsva_ext2(Y = true_data$Y, X = true_data$P, q = p)
  X_nnls <- apply(true_data$Y, 2, function(y) {lsei::nnls(a = true_data$P, b = y)$x})
  # X_pnnls <- apply(true_data$Y, 2, function(y) {lsei::pnnls(a = true_data$P, b = y, sum = 1)$x})
  
  ## if we know both X and Z
  P_new = cbind(true_data$Z, true_data$P)
  X_hat <- apply(true_data$Y, 2, function(y) {lsei::pnnls(a = P_new, b = y, k = p, sum = NULL)$x})[-seq(p), ]
  
  cor_dSVA <- mean(my_cor(t(true_data$X_star), t(X_dSVA)))
  cor_nnls <- mean(my_cor(t(true_data$X_star), t(X_nnls)))
  # cor_pnnls <- mean(my_cor(true_data$P_star, P_pnnls))
  cor_best <-  mean(my_cor(t(true_data$X_star), t(X_hat)))
  
  mse_dSVA <- my_mse(true_data$X_star, X_dSVA)
  mse_nnls <- my_mse(true_data$X_star, X_nnls)
  # mse_pnnls <- my_mse(true_data$P_star, P_pnnls)
  mse_best <- my_mse(true_data$X_star, X_hat)
  
  if (mse_best > 1e6) {
    result_matrix[b, ] <- rep(NaN, 6)
    ab_list[[bb]] <- true_data
    bb <- bb + 1
  } else {
    result_matrix[b, ] <- c(cor_dSVA, cor_nnls, cor_best, mse_dSVA, mse_nnls, mse_best)
  }
}

result_df <- as_tibble(cbind(1:B, result_matrix))
colnames(result_df) <- c("b", "cor_dSVA", "cor_nnls", "cor_known", "mse_dSVA", "mse_nnls", "mse_known")

par(mfrow = c(1, 2))
boxplot(x = result_df$cor_dSVA, result_df$cor_nnls, result_df$cor_known, xlab = "Method", ylab = "Cor", subset = result_df$mse_known < 1e6)
axis(1, at = c("1", "2", "3"), labels = c("dSVA", "NNLS", "Known"))
boxplot(result_df$mse_dSVA, result_df$mse_nnls, result_df$mse_known, xlab = "Method", ylab = "MSE", subset = result_df$mse_known < 1e6, log = "y")
axis(1, at = c("1", "2", "3"), labels = c("dSVA", "NNLS", "Known"))
```

There are some runs that do not converge, the reasons of which I am not sure at the moment. 